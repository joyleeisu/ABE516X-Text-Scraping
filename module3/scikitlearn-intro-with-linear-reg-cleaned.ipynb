{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Scikit-Learn with Linear Regression\n",
    "\n",
    "This tutorial is based off of:  https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html\n",
    "\n",
    "One of the best known packages that provides access to lots of machine learning algorithms in Python is Scikit-Learn. Scikit-Learn has a clean, uniform, and streamlined use of commands, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is straightforward.  Let's take a look at the basic commands in a scikit-learn analysis.  First, let's take a look at some data.  We are going to import the seaborn package, which is a statistical data visualization tool that extends beyond matplotlib.  It works well with numpy, pandas, and scikit-learn.  We'll also load a demo dataset that is internal to python called `iris`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the tabular output of this data.  Each row of the data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. In general, we will refer to the rows of the matrix as samples, and the number of rows as `n_samples`.\n",
    "\n",
    "Likewise, each column of the data refers to a particular quantitative piece of information that describes each sample. In general, we will refer to the columns of the matrix as features, and the number of columns as `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table layout can be thought of as a two-dimensional numerical array or matrix, which we will call the `features matrix`. For Scikit-learn analysis, by convention, this features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape [n_samples, n_features], and is most often contained in a NumPy array or a Pandas DataFrame.  \n",
    "\n",
    "The samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n",
    "\n",
    "The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases.\n",
    "\n",
    "In addition to the feature matrix `X`, we also generally work with a label or `target array`, which by convention we will usually call `y`. The target array is usually one dimensional, with length n_samples, and is generally contained in a NumPy array or Pandas Series. The target array may have continuous numerical values, or discrete classes/labels.  We will primarily be working with the common case of a one-dimensional target array.\n",
    "\n",
    "Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the `target array` is that it is usually the *quantity we want to predict from the data*: in statistical terms, it is the dependent variable. For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the species column would be considered the `target array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `seaborn`, you can quickly visualize your datasets - also see the great [Seaborn documentation](https://seaborn.pydata.org/) and some [tutorials](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in Scikit-Learn, we will extract the features matrix and target array from the DataFrame, which we can do using some of the Pandas DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic steps of Scikit-Learn analysis\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn are as follows:\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "4. Fit the model to your data by calling the fit() method of the model instance.\n",
    "5. Apply the Model to new data:\n",
    "    1. For supervised learning, often we predict labels for unknown data using the predict() method.\n",
    "    2.  For unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method.\n",
    "\n",
    "Let's see how this would work for a `linear regression`.  In this class, we do not have time to dive into statistical theory but rather how we can deploy statistical methods.  If you want to learn more about linear regression, I found this [blog](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/) to be helpful.\n",
    "\n",
    "To learn how to perform a linear regression, let's first create a dataset to build a model from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n",
    "Choose model hyperparameters by instantiating this class with desired values.\n",
    "\n",
    "Let's choose the linear regression model and tell it to fit the intercept.  The parameters for each model are well documented.  The parameters for linear regression are [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).  \n",
    "\n",
    "`fit_intercept : boolean, optional, default True`\n",
    "whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important point is that a class of model is not the same as an instance of a model.\n",
    "\n",
    "Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
    "\n",
    "Would we like to fit for the offset (i.e., y-intercept)?\n",
    "Would we like the model to be normalized?\n",
    "Would we like to preprocess our features to add model flexibility?\n",
    "What degree of regularization would we like to use in our model?\n",
    "How many model components would we like to use?\n",
    "\n",
    "These are examples of the important choices that must be made once the model class is selected. These choices are often represented as hyperparameters, or parameters that must be set before the model is fit to data. In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation. You have to know a bit about your model, your question, and your data in order to answer these questions.  There is rarely a one-size-fit-all solution.\n",
    "\n",
    "But back to our linear regression:\n",
    "For our linear regression example, we can instantiate the LinearRegression class and specify that we would like to fit the intercept using the fit_intercept hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data into a features matrix and target vector following the discussion above. Our current x from our dataset needs to be a two dimensional array, [n_samples, n_features].  Because we do not have multiple features for this data, we can make an empty n_features column.\n",
    "\n",
    "Our target vector to predict is `y` and is a 1D array with the shape matching the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to your data by calling the fit() method of the model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit() command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the fit() process have trailing underscores; for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, Scikit-Learn does not provide tools to draw conclusions from internal model parameters themselves: interpreting model parameters is much more a statistical modeling question than a machine learning question. Machine learning rather focuses on what the model predicts. If you would like to dive into the meaning of fit parameters within the model, other tools are available, including the [Statsmodels Python package](https://www.statsmodels.org/stable/examples/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, this can be done using the predict() method. For the sake of this example, our \"new data\" will be a grid of x values, and we will ask what y values the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to coerce these x values into a [n_samples, n_features] features matrix, after which we can feed it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results by plotting first the raw data, and then this model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
